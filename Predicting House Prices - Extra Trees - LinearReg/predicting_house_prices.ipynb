{
  "cells": [
    {
      "metadata": {
        "_uuid": "23162b22555db7b955451c266accd25c15621419"
      },
      "cell_type": "markdown",
      "source": "# Table of contents\n*  [Introduction](#section1) \n*  [Libraries](#section2) \n*  [Read in the data](#section3)\n*  [Feature engineering](#section4)\n    - [Missing values](#section5)\n        - [Numerical columns](#section6)\n        - [Text columns](#section7)\n    - [Adding new features](#section8)\n    - [Removing columns](#section9)\n        - [Irrelevant columns](#section10)\n        - [Columns that leak sale information](#section11)\n    - [Categorical features](#section12)\n*  [Feature selection](#section13)\n    - [Data preparation](#section14)\n    - [1. Correlations](#section15)\n    - [2. Univariate Selection](#section16)\n    - [3. Recursive Feature Elimination](#section17)\n    - [4. Extra Trees classifier - Feature Importance](#section18)\n*  [Modeling and testing](#section19)"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9e82ed1f7fe4ac399a3c91c292d6ffe6546ec019"
      },
      "cell_type": "markdown",
      "source": "by @antosnj\n\n-----\n<a id='section1'></a>\n# Introduction\nThis kernel aims to predict house sale prices using different linear regression models based on four feature selection techniques: Simple correlations, Univariate Selection, Recursive Feature Elimination and Extra Trees Classifier. The dataset consists of housing data for the city of Ames, Iowa, United States from 2006 to 2010. \n\n- Info about why the data was collected can be found at http://jse.amstat.org/v19n3/decock.pdf.\n- Info about the dataset columns at http://jse.amstat.org/v19n3/decock/DataDocumentation.txt"
    },
    {
      "metadata": {
        "_uuid": "628c0cc5b11b7b786cbc2208d3f512fc9699de7c"
      },
      "cell_type": "markdown",
      "source": "----\n<a id='section2'></a>\n# Libraries"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f128adf6829f85836f4ea596c7ebcc883ea45cbf"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import cross_val_predict\n\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "064ce386cdad81003741a79cf537381d3a0f8c2c"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section3'></a>\n# Read in the data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d408fac3bde66180abe0f66e10e6e9ecd0da482f"
      },
      "cell_type": "code",
      "source": "housing_data = pd.read_csv('../input/ameshousing/AmesHousing/AmesHousing.tsv', delimiter=\"\\t\")\nprint(housing_data.shape)\nhousing_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36c06c2b058b8396245bb83b01dd2d5bb2a064bd"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section4'></a>\n# Feature engineering"
    },
    {
      "metadata": {
        "_uuid": "963fdcb44591cd4f8181ec6e37d77e83d8175a70"
      },
      "cell_type": "markdown",
      "source": "<a id='section5'></a>\n## Missing values\n---\n<a id='section6'></a>\n### Numerical columns\nThe data contains 2930 observations. Let's start out by analyzing which numerical columns have less than 5% of missing values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65cf3db9534be4c6bb4401ac0b38485da5d8db2a"
      },
      "cell_type": "code",
      "source": "numerical_cols = housing_data.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns\nnull_values = housing_data[numerical_cols].isnull().sum()\nless_than_5pct = null_values[(null_values != 0) & (null_values < 0.05*2930)]\nmost_common = housing_data[less_than_5pct.index].mode()\n\nprint(less_than_5pct)\nprint(\"\\nMOST COMMON VALUES:\\n\", most_common)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a37d2aef72eddf70d7d426db9ed85025e325a6b1"
      },
      "cell_type": "markdown",
      "source": " and fill those out with the most popular value of each column."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "afe4ab44de59164f514ed775a0a855f4df063f3c"
      },
      "cell_type": "code",
      "source": "housing_data.loc[:, less_than_5pct.index] = housing_data.loc[:, less_than_5pct.index].fillna(most_common.loc[0])\nhousing_data[less_than_5pct.index].isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "416fdcbc52d7cd9e15f5501754a2e001e6aa986c"
      },
      "cell_type": "markdown",
      "source": "---\nIt turns out that there are only two columns that contain more than 5% of missing values in the entire dataset. In particular, they're missing a 16.7% and a 5.4% of the total number of rows respectively, as seen below."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e0c42c0a90e35c49ac935740b51e62d517e3864"
      },
      "cell_type": "code",
      "source": "more_than_5pct = null_values[null_values >= 0.05*2930]\ncols = more_than_5pct.index.tolist() + ['SalePrice']\n\nprint(\"\\nPERCENTAGE OF MISSING VALUES:\")\nprint(more_than_5pct/2930)\nprint(\"\\nCORRELATION:\")\nprint(housing_data[cols].corr())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f65109a473cb51dfc603fd2154b544c4e61be1c"
      },
      "cell_type": "markdown",
      "source": "'Lot Frontage' has more than 15% of missing values, shows a low correlation with 'SalePrice' (the target column we want to predict) and won't help us create new features a priori, so I'll drop the column. However, 'Garage Yr Blt' shows more correlation and might be useful later on, so let's fill out the 5.4% missing values with the most common value in the column as well."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e258f34792c174fb5ca5b4275480f3cf84302113"
      },
      "cell_type": "code",
      "source": "housing_data.drop(columns='Lot Frontage', inplace=True)\nmost_common_value = housing_data['Garage Yr Blt'].mode()\nhousing_data.loc[:, 'Garage Yr Blt'] = housing_data.loc[:, 'Garage Yr Blt'].fillna(most_common_value)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "377e890e73cd5a28be1bc1a9f3fa7060a133f573"
      },
      "cell_type": "markdown",
      "source": "<a id='section7'></a>\n### Text columns\nSimilarly, let's see which text columns have more than 5% of missing values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc77c5c588d2559f6ec3d36dfdd83a72a6b0f143"
      },
      "cell_type": "code",
      "source": "text_cols = housing_data.select_dtypes(include=['object']).columns\nnull_values = housing_data[text_cols].isnull().sum()\nmore_than_5pct = null_values[null_values > 0.05*2930]\nmore_than_5pct",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "99ce3d5a5f4a53c2986d2c07a8b87618800ef665"
      },
      "cell_type": "markdown",
      "source": "Given the number of missing values in 'Alley', 'Fireplace Qu', 'Pool QC', 'Fence' and 'Misc Feature', I will drop them. For 'Garage Type', 'Garage Finish', 'Garage Qual' and 'Garage Cond', it may work best if we just drop the rows where the missing values happen, since there're not as many."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fe762673aa24705b9fdd35e9f73cf7314371b33"
      },
      "cell_type": "code",
      "source": "housing_data.drop(['Alley', 'Fireplace Qu', 'Pool QC', 'Fence', 'Misc Feature'], axis=1, inplace=True)\nhousing_data.dropna(subset=['Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond'], inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "25413ca957673e349218ebf27f23b7a190e423e4"
      },
      "cell_type": "markdown",
      "source": "<a id='section8'></a>\n## Adding new features\nThe following columns could be combined to get new useful features:\n- 'Year Built' - 'Yr Sold': Compute the age (years) of the house when at the time it was sold.\n- 'Year Remod/Add' - 'Yr Sold': Compute the years passed between the remodel date and the year it was sold.\n\nAfter creating the new features, we can drop them."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23a854ac728f0b8dc8b62cd1bd26890c5f201306"
      },
      "cell_type": "code",
      "source": "housing_data['age_when_sold'] = housing_data['Yr Sold'] - housing_data['Year Built']\nhousing_data['years_remodeled'] = housing_data['Yr Sold'] - housing_data['Year Remod/Add']\n\nfor col in ['Year Built', 'Year Remod/Add']:\n    housing_data.drop(col, axis=1, inplace=True)\n    \nprint(housing_data['age_when_sold'].value_counts())\nprint(housing_data['years_remodeled'].value_counts())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "509774c289d8472789535e21243804cf1cfb426d"
      },
      "cell_type": "markdown",
      "source": "Looks like there're some negative values. Since we're representing age, we need to get rid of the few rows where those happen. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a239ea4f5b5ebb8cd3b7c37604a471047b2be17a"
      },
      "cell_type": "code",
      "source": "negative_age_index = housing_data.loc[housing_data['age_when_sold'] < 0].index.tolist()\nnegative_remodeled_index = housing_data.loc[housing_data['years_remodeled'] < 0].index.tolist()\n\nnegative_rows = negative_age_index + negative_remodeled_index\nhousing_data.drop(negative_rows, axis=0, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59b511ca12aad8d59af426af5b1b5c8f8e53936a"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section9'></a>\n## Removing columns"
    },
    {
      "metadata": {
        "_uuid": "251cfd72fdd0c51ea98d5a355eac644288fd2aae"
      },
      "cell_type": "markdown",
      "source": "<a id='section10'></a>\n### Irrelevant columns\nSome of the columns are not going to be useful for the model. In particular, looking at the documentation we could exclude the following:\n- 'Order': Observation number\n- 'PID': Parcel identification number."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f58b09d12a204bbd97ee0877110d9702a12c205"
      },
      "cell_type": "code",
      "source": "housing_data.drop(['Order', 'PID'], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2690bfc18f44fd9dcb3d0906da10bbc15c4be73d"
      },
      "cell_type": "markdown",
      "source": "<a id='section11'></a>\n### Columns that leak sale information\nThese columns contain information about the sale itself. The new houses our model will be used for to predict prices in the future won't have that information, since they haven't been sold yet, so we can't use it as part of our feature selection. In the dataset, these include:\n\n- 'Sale Type': Type of sale\n- 'Yr Sold': Year Sold (YYYY)\n- 'Mo Sold': Month Sold (MM)\n- 'Sale Condition': Condition of sale"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38d7dd944e259f4b6cfa485f81bd7f31b2ba896e"
      },
      "cell_type": "code",
      "source": "housing_data.drop(['Sale Type', 'Yr Sold', 'Mo Sold', 'Sale Condition'], axis=1, inplace=True)\n\n# See how the data looks like after cleaning and transforming features\nhousing_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ac690e1af01aab1bb279f6cc4653a0ee306ffca"
      },
      "cell_type": "markdown",
      "source": "<a id='section12'></a>\n## Categorical features\nCandidates to be converted to categorical columns are the nominal ones. Looking at the documentation, they are the following:\n\n- PID: Parcel identification number\n- MS Zoning: Identifies the general zoning classification of the sale\n- Street: Type of road access to property\n- Alley: Type of alley access to property\n- Lot Shape: General shape of property\n- Land Contour: Flatness of the property\n- Utilities: Type of utilities available\n- Lot Config: Lot configuration\n- Land Slope: Slope of property\n- Neighborhood: Physical locations within Ames city limits (map available)\n- Condition 1: Proximity to various conditions\n- Condition 2: Proximity to various conditions (if more than one is present)\n- Bldg Type: Type of dwelling\n- House Style: Style of dwelling\n- Roof Style: Type of roof\n- Roof Matl: Roof material\n- Exterior 1st: Exterior covering on house\n- Exterior 2nd: Exterior covering on house (if more than one material)\n- Mas Vnr Type: Masonry veneer type\n- Foundation: Type of foundation\n- Heating: Type of heating\n- Central Air: Central air conditioning\n- Garage Type: Garage location\n- Misc Feature: Miscellaneous feature not covered in other categories\n- Sale Type: Type of sale\n- Sale Condition: Condition of sale\n\nSome of them have been dropped earlier, so we don't have to consider them anymore. Let's also see how many unique values these columns have, since if they're too many, then too many columns will need to be added back to the data frame if we dummy the code."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "478b664121e42676dc869a85110c9eb862683962"
      },
      "cell_type": "code",
      "source": "categorical_cols = ['PID', 'MS Zoning', 'Street', 'Alley', 'Lot Shape', 'Land Contour', \n        'Utilities', 'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1', \n        'Condition 2', 'Bldg Type', 'House Style', 'Roof Style', 'Roof Matl', \n        'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type', 'Foundation', 'Heating', \n        'Central Air', 'Garage Type', 'Misc Feature', 'Sale Type', 'Sale Condition']\n\n# Check dropped columns\nprint(\"DROPPED COLS:\\n\")\ndropped_cols = []\nfor col in categorical_cols:\n    if col not in housing_data.columns:\n        print(col)\n        dropped_cols.append(col)\n\n# Remove them from 'categorical_cols'\nfor col in dropped_cols:\n    categorical_cols.remove(col)\n\n# Check number of unique values\nhousing_data[categorical_cols].describe().loc['unique'].sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13fa5b9dbfd52ace12ecb1f4c49ccaa3effe2587"
      },
      "cell_type": "markdown",
      "source": "Based on the results and to set a cutoff, let's exclude 'Neighborhood', 'Exterior 1st' and 'Exterior 2nd', which have more than 10 unique values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99b7380a63cc97cd75c3e7b58b6441103e186627"
      },
      "cell_type": "code",
      "source": "for col in ['Neighborhood', 'Exterior 1st', 'Exterior 2nd']:\n    categorical_cols.remove(col)\n    \nfor col in categorical_cols:\n    housing_data[col] = housing_data[col].astype('category')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c9137f5a50e9256b59056296f0e8152bdf81940"
      },
      "cell_type": "markdown",
      "source": "---\nWe don't want columns that have a few unique values but too many values in the column belong to a specific category either (say more than 90% of the total number of values in the column), since columns with low variance mean no variability in the data for the model to capture."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "115932bc5b42a203a115727207d042f178bfb758"
      },
      "cell_type": "code",
      "source": "low_variance_cols = []\n\nfor col in categorical_cols:\n    col_value_count = housing_data[col].value_counts()\n    pct = col_value_count/sum(col_value_count)\n    \n    # Find any column that has one value representing 90% of the total number of values\n    if max(pct) > 0.9:\n        print(pct.name)\n        low_variance_cols.append(pct.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5950f311917bafb4190a143c7efd7c47f337a3b8"
      },
      "cell_type": "markdown",
      "source": "Let's drop those from 'categorical_cols'."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b6139ec2c2e98375de0e870779c8c59eef8038d"
      },
      "cell_type": "code",
      "source": "for col in low_variance_cols:\n    categorical_cols.remove(col)\n\n# See how 'housing_data' final categorical columns look like \nhousing_data[categorical_cols].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4aa33a568079a318ac4c4db6c701d6adeb863220"
      },
      "cell_type": "markdown",
      "source": "Finally, let's create the new dummy variables using the categorical columns."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25e45e265d1e0d5f234cd2ad650cce1d6eb7de0d"
      },
      "cell_type": "code",
      "source": "print(\"Before:\", housing_data.shape)\n\nfor col in categorical_cols:\n    dummy_col = pd.get_dummies(housing_data[col])\n    housing_data = pd.concat([housing_data, dummy_col], axis=1)\n    housing_data.drop(col, axis=1, inplace=True)\n\nprint(\"After:\", housing_data.shape)\nhousing_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2357137334699ce8cc048bf1801805f2a9992704"
      },
      "cell_type": "markdown",
      "source": "51 new columns were successfully added to the data frame. Before proceeding to feature selection, let's take another look at any correlations with 'SalePrice' higher than 0.5"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d5d8e0a240d35c3bc5fcef82de14548b060fe12"
      },
      "cell_type": "code",
      "source": "corr = housing_data.corr()['SalePrice'].sort_values()\ncorr[corr > 0.5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2bf31f0b36d0f87459a0771b44b1f0d8fab89997"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section13'></a>\n# Feature Selection\nFor feature selection, I am going to take 4 different approaches and contrast their results, testing a linear regression model. To start out with, I will select 10 features, but I will experiment with different number of features later on when testing different models:\n- Correlations\n- Univariate selection\n- Recursive Feature Elimination\n- Principal Component Analysis\n- Feature Importance\n\nFor reference about some of the techniques I've used see https://machinelearningmastery.com/feature-selection-machine-learning-python/."
    },
    {
      "metadata": {
        "_uuid": "ac525a069802818991ade140027234d2df9d2b92"
      },
      "cell_type": "markdown",
      "source": "<a id='section14'></a>\n## Data preparation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b17f6ec3e02478d6ba4237c244ca81e22ff2226"
      },
      "cell_type": "code",
      "source": "# Select numerical features\nnumerical_data = housing_data.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\nX = numerical_data.loc[:, numerical_data.columns != 'SalePrice']\n\n# Target column\ny = numerical_data['SalePrice']\n\ncol_names = housing_data.columns.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0dcb2579c9779ee52aa132bcfeb4c402d98fa5d7"
      },
      "cell_type": "markdown",
      "source": "<a id='section15'></a>\n## 1. Correlations"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba8e7b52892e8a237cc96b2b7e98e6f27767a5c0"
      },
      "cell_type": "code",
      "source": "# Sorted correlations with target column 'SalePrice'\nnumerical_data = housing_data.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\nsorted_corrs = numerical_data.corr()['SalePrice'].sort_values()\nstrong_corrs_features = sorted_corrs[sorted_corrs > 0.5]\n\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(housing_data[strong_corrs_features.index].corr())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eae9994964050c41b4f263c4a6c73bda02fa7cd4"
      },
      "cell_type": "code",
      "source": "print(\"\\nSTRONG CORRELATIONS WITH 'SalePrice':\\n\\n\", strong_corrs_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa1c7521e372d56ec1a06156dfd4d45312e95ee0"
      },
      "cell_type": "markdown",
      "source": "<a id='section16'></a>\n## 2. Univariate Selection\nFor this technique, let's consider the chi squared (chi^2) statistical test for non-negative features to select 10 of the best features."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "2f638d0a6ef4de2a2bde609b2aa680dcd4b626a6"
      },
      "cell_type": "code",
      "source": "# Feature extraction\nselector = SelectKBest(score_func=chi2, k=10)\nfit = selector.fit(X, y)\n\n# Summarize scores\ntransformed_features = fit.transform(X)\n\nmask = selector.get_support() \nnew_features = [] # The list of K best features\n\nfor bool, feature in zip(mask, col_names):\n    if bool:\n        new_features.append(feature)\n\n# df with 10 best features using univariate selection\ndata_univ_selection = pd.DataFrame(transformed_features, columns=new_features)\n\nprint(\"\\n10 BEST FEATURES:\\n\\n\", new_features)\nnp.set_printoptions(precision=3)\nprint(\"\\nSCORES:\\n\\n\", fit.scores_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c027649718a807fe636c70bff28c88cfe50ea0bd"
      },
      "cell_type": "markdown",
      "source": "<a id='section17'></a>\n## 3. Recursive Feature Elimination"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e11eaa98a2a15c1ea3bdf3b71fcc51a774f7afa0"
      },
      "cell_type": "code",
      "source": "# Feature extraction\nmodel = LinearRegression()\nrfe = RFE(model, 10)\nfit = rfe.fit(X, y)\n\nindexes = [i for i, x in enumerate(fit.support_) if x]\nfeature_names = X.columns[indexes].tolist()\n\n# df with 10 best features using RFE\ndata_RFE = X[feature_names]\n\n# Summarize components\nprint(\"Num Features:\", fit.n_features_)\nprint(\"\\nSelected Features:\\n\", fit.support_) \nprint(\"\\nFeature Ranking:\\n\", fit.ranking_)\nprint(\"\\nFeature names:\\n\", feature_names)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c099521e57d496ebdfb59a2eb782b49e7537f8cc"
      },
      "cell_type": "markdown",
      "source": "<a id='section18'></a>\n## 4. Extra Trees classifier - Feature Importance"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6fa141009581a9f8cea0387f26ac04f518ae5b4"
      },
      "cell_type": "code",
      "source": "# Feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\n\n# Select 10 best features\nimportances = model.feature_importances_.tolist()\nmax_importances = np.sort(importances)[::-1][0:10]\n\nbest_indexes = []\nfor x in max_importances:\n    best_indexes.append(importances.index(x))\n    \nbest_features = X.columns[best_indexes].tolist()\n\n# df with 10 best features using univariate selection\ndata_ET = X[best_features]\n\nprint(\"\\n10 BEST FEATURES:\\n\\n\", best_features)\nprint(\"\\nIMPORTANCE VALUES:\\n\\n\", max_importances)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aec6b033b118a269b71dd89359358ac6ee88829b"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section19'></a>\n# Modeling and Testing\nLet's put it all together in a function in such a way we can test different models easily, starting with a feature selection function that gathers all feature selection techniques discussed before."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1d143e5e03d231343bb376d720bc4c1171d6035"
      },
      "cell_type": "code",
      "source": "# ----------------------------------------------------------\n#                   FEATURE SELECTION\n# ----------------------------------------------------------\n# Parameters:\n#\n# df: data frame containing data\n# target_col: target column \n# num_features: number of features to be used\n# technique: technique to be used for feature selection:\n#            'CORR': Select features with correlation with target column higher than 0.5\n#            'US': Univariate Selection\n#            'RFE': Recursive Feature Elimination\n#            'ET': Extra Trees classifier - Feature importance\n# \n# Returns:\n#\n# df_technique (DataFrame): training data containing selected features using the input technique\n# best_features (list): names of selected features\n\n\ndef feature_selection(df, target_col, num_features, technique):\n    \n    # Select numerical features\n    numerical_data = df.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\n    X = numerical_data.loc[:, numerical_data.columns != target_col]\n\n    # Target column\n    y = df[target_col]\n    \n    # Select features based on input technique:\n    if technique == 'CORR':\n        sorted_corrs = numerical_data.corr()[target_col].sort_values()\n        best_features = sorted_corrs[(sorted_corrs > 0.5) & (sorted_corrs.index != target_col)].index.tolist()\n        \n        df_technique = X[best_features]\n        \n        \n    elif technique == 'US':\n        selector = SelectKBest(score_func=chi2, k=num_features)\n        fit = selector.fit(X, y)\n        transformed_features = fit.transform(X)\n\n        mask = selector.get_support() \n        col_names = df.columns.values\n        best_features = [] # The list of K best features\n    \n        for bool, feature in zip(mask, col_names):\n            if bool:\n                best_features.append(feature)\n\n        df_technique = pd.DataFrame(transformed_features, columns=best_features)\n        \n        \n    elif technique == 'RFE':\n        model = LinearRegression()\n        rfe = RFE(model, num_features)\n        fit = rfe.fit(X, y)\n\n        indexes = [i for i, x in enumerate(fit.support_) if x]\n        best_features = X.columns[indexes].tolist()\n\n        df_technique = X[best_features]\n\n    \n    elif technique == 'ET':\n        model = ExtraTreesClassifier()\n        model.fit(X, y)\n        \n        importances = model.feature_importances_.tolist()\n        max_importances = np.sort(importances)[::-1][0:num_features]\n\n        best_indexes = []\n        for x in max_importances:\n            best_indexes.append(importances.index(x))\n\n        best_features = X.columns[best_indexes].tolist()\n        df_technique = X[best_features]\n        \n        \n    return(df_technique, best_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d19a937390ff010d5bbb36b040264ab734c2416"
      },
      "cell_type": "markdown",
      "source": "---\nNext, let's define a train_and_test function based on a linear regression model and _k_-fold cross validation, that makes predictions and returns the corresponding RMSE value depending on the number of features, number of folds and the feature selection technique."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "519c9a5758950da97e48260ab92978b0e0580535"
      },
      "cell_type": "code",
      "source": "def train_and_test(data, target_col, num_features, k, feature_sel_technique):\n    \n    X, features = feature_selection(data, target_col, num_features, feature_sel_technique)\n    y = data[target_col]\n    \n    # If number of folds equals 1\n    if k == 1:\n        data = data.sample(frac=1, )\n        \n        X_train = X[:1460]\n        y_train = y[:1460]\n        X_test = X[1460:]\n        y_test = y[1460:]\n        \n        lir = LinearRegression()\n        \n        lir.fit(X_train, y_train)\n        new_prices_1 = lir.predict(X_test)        \n        mse_one = mean_squared_error(y_test, new_prices_1)\n        rmse_one = np.sqrt(mse_one)\n        \n        lir.fit(X_test, y_test)\n        new_prices_2 = lir.predict(X_train)        \n        mse_two = mean_squared_error(y_train, new_prices_2)\n        rmse_two = np.sqrt(mse_two)\n        \n        avg_rmse = np.mean([rmse_one, rmse_two])\n        \n        return(avg_rmse, new_prices_1, features)\n    \n\n\n    # Model\n    # Train\n    lir = LinearRegression()\n    lir.fit(X, y)\n    kf = KFold(n_splits=k, shuffle=True, random_state=1)\n    \n    # Predict\n    new_prices = cross_val_predict(lir, X, y, cv=kf)\n\n    # Test - Evaluate RMSE\n    mses = cross_val_score(lir, X, y, scoring='neg_mean_squared_error', cv=kf)\n    avg_rmse = np.mean(np.sqrt(np.absolute(mses)))\n    \n    return(avg_rmse, new_prices, features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2873477cd722bced64a583245f6029c36ff9081e"
      },
      "cell_type": "markdown",
      "source": "---\nLet's test different linear regression models by changing the number of features to be selected, the technique to be used to select such features, and the parameter _k_ to control the type of (_k_-fold) cross validation that occurs, and save the one that returns the lowest RMSE to 'best_rmse', and the one that returns the lowest RMSLE to 'best_rmsle'."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49b0c2beafec0d14f86972b59ddc2d6443e5a13c"
      },
      "cell_type": "code",
      "source": "num_features = range(1, 30)\nn_folds = range(1, 11)\ntechniques = ['CORR', 'US', 'RFE', 'ET']\ntarget_col = 'SalePrice'\n\n# Define a RMSLE function\ndef rmsle(y_pred, y_test) : \n    assert len(y_test) == len(y_pred)\n    return np.sqrt(np.mean((np.log(1+y_pred) - np.log(1+y_test))**2))\n\ncurrent_rmse = math.inf\ncurrent_rmsle = math.inf\nbest_rmse = {}\nbest_rmsle = {}\ny_test = housing_data[1460:]\n\nfor k in n_folds:\n    for technique in techniques:\n        for n_features in num_features:\n            avg_rmse, new_prices, features = train_and_test(housing_data, target_col, n_features, k, technique)\n            \n            if k == 1:\n                rmsle_v = rmsle(new_prices, y_test[target_col])\n            else:\n                rmsle_v = rmsle(new_prices[1460:], y_test[target_col])\n\n            \n            if avg_rmse < current_rmse:\n                best_rmse['RMSE'] = avg_rmse\n                best_rmse['new_prices'] = new_prices\n                best_rmse['features'] = features  \n                best_rmse['technique'] = technique  \n                best_rmse['num_features'] = n_features  \n                best_rmse['n_folds'] = k\n                \n                current_rmse = avg_rmse\n                \n            if rmsle_v < current_rmsle:\n                best_rmsle['RMSLE'] = rmsle_v\n                best_rmsle['new_prices'] = new_prices\n                best_rmsle['features'] = features  \n                best_rmsle['technique'] = technique  \n                best_rmsle['num_features'] = n_features  \n                best_rmsle['n_folds'] = k\n                \n                current_rmsle = rmsle_v\n                \nprint(best_rmse)\nprint(best_rmsle)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c6aea44492892e22191343471fc9683e4795ea6"
      },
      "cell_type": "markdown",
      "source": "### Best results:\n---\n- **RMSLE: 0.160765**\n    - _Feature selection technique_: Extra Trees Classifier - Feature Importance\n    - _Number of folds cross validation (k)_: 1\n    - _Number of features used_: 21\n    - _Features used/selected_: ['Bsmt Unf SF', 'Gr Liv Area', 'Lot Area', 'Garage Area', 'Total Bsmt SF', 'years_remodeled', '1st Flr SF', 'age_when_sold', 'Garage Yr Blt', 'BsmtFin SF 1', 'Wood Deck SF', 'Open Porch SF', 'TotRms AbvGrd', 'Mas Vnr Area', 'Overall Qual', 'Overall Cond', 'Bedroom AbvGr', '2nd Flr SF', 'Fireplaces', 'MS SubClass', 'Bsmt Full Bath']\n    \n    \n- **RMSE: 31293.8494**\n    - _Feature selection technique_: Extra Trees Classifier - Feature Importance\n    - _Number of folds cross validation (k)_: 8\n    - _Number of features used_: 27\n    - _Features used/selected_: ['Lot Area', 'Garage Area', '1st Flr SF', 'Gr Liv Area', 'Bsmt Unf SF', 'years_remodeled', 'Total Bsmt SF', 'age_when_sold', 'Garage Yr Blt', 'BsmtFin SF 1', 'Open Porch SF', 'Wood Deck SF', 'TotRms AbvGrd', 'Mas Vnr Area', 'Overall Qual', '2nd Flr SF', 'Overall Cond', 'Bedroom AbvGr', 'MS SubClass', 'Fireplaces', 'Bsmt Full Bath', 'Enclosed Porch', 'Garage Cars', 'BsmtFin SF 2', 'Full Bath', 'Half Bath', 'Screen Porch']\n---\n**Note:** For iterations where less features were used, as an example, for the case of 14 features it returned a RMSLE of 0.171812 (k=1 folds) and a RMSE of 32888.7108 (k=8 folds) using Recursive Feature Elimination (RFE)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b47639767add5f5ca4ed3582bf4c7ee07afc76b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}