{
  "cells": [
    {
      "metadata": {
        "_uuid": "9ffd83e2b5b6bccad0c1f3cd20338ec9f931b173"
      },
      "cell_type": "markdown",
      "source": "# Table of contents\n*  [Introduction](#section1) \n*  [Read in the data](#section2)\n*  [Feature engineering](#section3)\n    - [Pandas date type](#section4)\n    - [New indicators](#section5)\n    - [NaN values](#section6)\n*  [Train/Test split](#section7)\n*  [Linear model](#section8)\n*  [Random Forest](#section9)\n\nby @antosnj"
    },
    {
      "metadata": {
        "_uuid": "b72fdd0cf8d8d3f749df744e56013d5ed212b217"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section1'></a>\n# Introduction\nThis project aims to predict the S&P500 Index using feature engineering, creating new features based on the historical data, and building both a linear regression and Random Forest model. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d1c82e3b944ddd8c129c1add2e00bfbdff71e5c"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4d437069e8d39460a9566c18289a5238a92b5606"
      },
      "cell_type": "markdown",
      "source": "<a id='section2'></a>\n# Read in the data\n\nThe dataset contains historical data on the price of the S&P500 Index. The columns are:\n\n- Date: The date of the record.\n- Open: The opening price of the day (when trading starts).\n- High: The highest trade price during the day.\n- Low: The lowest trade price during the day.\n- Close: The closing price for the day (when trading is finished).\n- Volume: The number of shares traded.\n- Adj Close: The daily closing price, adjusted retroactively to include any corporate actions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22e84c476ee3584a3150e64624df53853a48ce9d"
      },
      "cell_type": "code",
      "source": "sphist = pd.read_csv('../input/sphist.csv')\nprint(sphist.describe())\nprint(\"\\ndf shape: \", sphist.shape)\nsphist.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d57a289cf8e727c19d93627972e539076aa4c6cc"
      },
      "cell_type": "markdown",
      "source": "<a id='section3'></a>\n# Feature engineering\n<a id='section4'></a>\n## Pandas date type"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aec037ca19ef68538df7057f8a86bf8a3d320eae"
      },
      "cell_type": "code",
      "source": "# Convert 'Date' column to Pandas date type\nsphist['Date'] = pd.to_datetime(sphist['Date'])\n\n# Sort df by that column\nsphist.sort_values(by=['Date'], inplace=True)\nsphist.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "55ccd09516beb4a9b7d5a84a7edb28e9a434033d"
      },
      "cell_type": "markdown",
      "source": "<a id='section5'></a>\n## New indicators\n\nGiven the nature of the stock market, in order to prevent injecting future knowledge into the model, let's create indicators based on the past.\n\n- 1) Average price for the last 5 days\n- 2) Average price for the last 365 days\n- 3) Ratio between the average price for the past 5 days, and the average price for the past 365 days.\n- 4) Standard deviation of the price for the last 5 days\n- 5) Standard deviation of the price for the last 365 days\n- 6) Ratio between the standard deviation for the past 5 days, and the standard deviation for the past 365 days.\n- 7) The average volume over the past five days.\n- 8) The average volume over the past year.\n- 9) The ratio between the average volume for the past five days, and the average volume for the past year.\n- 10) The ratio between the lowest price in the past year and the current price."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf45260c23c3d391ac9b8361acfc9d17b5cb1f85"
      },
      "cell_type": "code",
      "source": "# Add new indicators to each observation:\n# 1 \nsphist['avg_price_5'] = sphist['Close'].rolling(5).mean()\nsphist['avg_price_5'] = sphist['avg_price_5'].shift() # Avoid using current day's price by reindexing\n\n# 2\nsphist['avg_price_365'] = sphist['Close'].rolling(365).mean()\nsphist['avg_price_365'] = sphist['avg_price_365'].shift() # Avoid using current day's price by reindexing\n\n# 3\nsphist['avg_price_5_365'] = sphist['avg_price_5'] / sphist['avg_price_365']\n\n# 4\nsphist['std_price_5'] = sphist['Close'].rolling(5).std()\nsphist['std_price_5'] = sphist['std_price_5'].shift() # Avoid using current day's price by reindexing\n\n# 5\nsphist['std_price_365'] = sphist['Close'].rolling(365).std()\nsphist['std_price_365'] = sphist['std_price_365'].shift() # Avoid using current day's price by reindexing\n\n# 6\nsphist['std_price_5_365'] = sphist['std_price_5'] / sphist['std_price_365']\n\n# 7 \nsphist['avg_volume_5'] = sphist['Volume'].rolling(5).mean()\nsphist['avg_volume_5'] = sphist['avg_volume_5'].shift() # Avoid using current day's price by reindexing\n\n# 8\nsphist['avg_volume_365'] = sphist['Volume'].rolling(365).mean()\nsphist['avg_volume_365'] = sphist['avg_volume_365'].shift() # Avoid using current day's price by reindexing\n\n# 9\nsphist['avg_volume_5_365'] = sphist['avg_volume_5'] / sphist['avg_volume_365']\n\n# 10\nmin_last_year = sphist['Close'].rolling(365).min()\nsphist['last_min_current_ratio'] = min_last_year / sphist['Close']\nsphist['last_min_current_ratio'] = sphist['last_min_current_ratio'].shift()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61b0049bd0004b65e05873e80a699180adb444b9"
      },
      "cell_type": "markdown",
      "source": "<a id='section6'></a>\n## NaN values\nSince the new indicators require data from the previous 5 and 365 days, after adding them, considering the first observation corresponds to 1950-01-03, we need to:\n\n- Remove any rows from the DataFrame that fall before 1951-01-03\n- Remove any rows with NaN values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf8bae8100038727907e5e9266ce489b9cc17117"
      },
      "cell_type": "code",
      "source": "print(\"# of observations before: \", sphist.shape[0])\nprint(\"NaN values before: \\n\\n\", sphist.isnull().sum())\n\nsphist = sphist[sphist['Date'] > datetime(year=1951, month=1, day=2)]\nsphist.dropna(axis=0, inplace=True)\n\nprint(\"\\n# of observations after: \", sphist.shape[0])\nprint(\"NaN values after: \\n\\n\", sphist.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2031a3d81ad31540d9ecc235fc9aad84bd7bddd0"
      },
      "cell_type": "markdown",
      "source": "<a id='section7'></a>\n# Train/Test split\n\n- Training set: Observations up to 2013-01-01\n- Test set: Observations after 2013-01-01"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21666086ddf30565d0a0eaeec9265d1e355b484a"
      },
      "cell_type": "code",
      "source": "train = sphist[sphist[\"Date\"] < datetime(year=2013, month=1, day=1)]\ntest = sphist[sphist[\"Date\"] >= datetime(year=2013, month=1, day=1)]\n\nprint(\"Train: \", train.shape)\nprint(\"Test: \", test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6d849131202db5de5db0890cc465516c6b5ddfb6"
      },
      "cell_type": "markdown",
      "source": "<a id='section8'></a>\n# Linear model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cc3d239f8aa47ec94638b214b8a02943c70022d"
      },
      "cell_type": "code",
      "source": "# Sorted correlations with target column 'Close'\nsorted_corrs = sphist.corr()['Close'].sort_values()\n\nprint(sorted_corrs)\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(sphist[sorted_corrs.index].corr())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c751043f94ebc7eb354e29f3649ec7ed1783e4e"
      },
      "cell_type": "code",
      "source": "features = ['avg_price_5', 'avg_price_365', 'avg_price_5_365', 'std_price_5', \n            'std_price_365', 'std_price_5_365', 'avg_volume_5', 'avg_volume_365', \n            'avg_volume_5_365', 'last_min_current_ratio']\n\nX_train = train[features]\ny_train = train['Close']\n\nX_test = test[features]\ny_test = test['Close']\n\n# Train\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Predict\nclosing_price_pred_lr = lr.predict(X_test)\n\n# --------------------------------------------------\n# Performance metrics\n# --------------------------------------------------\n\n# Calculate MSE\nmse_lr = mean_squared_error(y_test, closing_price_pred_lr)\n\n# Calculate the absolute errors and MAPE\nerrors_lr = abs(closing_price_pred_lr - y_test)\nmape_lr = 100 * (errors_lr / y_test)\n\n# MAE\nmae_lr = round(np.mean(errors_lr), 2)\n\n# Accuracy\naccuracy_lr = 100 - np.mean(mape_lr)\n\nprint(\"-----------------\\nLinear regression\\n-----------------\")\nprint(\"MSE: \", mse_lr)\nprint(\"MAE: \", mae_lr, \"degrees\")\nprint('Accuracy:', round(accuracy_lr, 2), '%.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb5f1dba5cddcfe7f6ba745f6f00a1499389c3b5"
      },
      "cell_type": "markdown",
      "source": "<a id='section9'></a>\n# Random Forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e1ef053cb66a0822fd98b2ff919cf7360272509"
      },
      "cell_type": "code",
      "source": "rf = RandomForestRegressor(n_estimators=150, random_state=1, min_samples_leaf=2)\n\n# Train \nrf.fit(X_train, y_train)\n\n# Predict\nclosing_price_pred_rf = rf.predict(X_test)\n\n# --------------------------------------------------\n# Performance metrics\n# --------------------------------------------------\n\n# Calculate the absolute errors and MAPE\nerrors_rf = abs(closing_price_pred_rf - y_test)\nmape_rf = 100 * (errors_rf / y_test)\n\n# MAE\nmae_rf = round(np.mean(errors_rf), 2)\n\n#Â Accuracy\naccuracy_rf = 100 - np.mean(mape_rf)\n\nprint(\"-----------------\\nRandom Forest\\n-----------------\")\nprint(\"MAE: \", mae_rf, \"degrees\")\nprint('Accuracy:', round(accuracy_rf, 2), '%.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a211222f6cd68ab4b28ba40024ee5ab87bfb986b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}