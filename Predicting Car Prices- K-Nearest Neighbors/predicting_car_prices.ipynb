{
  "cells": [
    {
      "metadata": {
        "_uuid": "92be5795cbe046575d3a3ffb93c75a92e3984fb9"
      },
      "cell_type": "markdown",
      "source": "# Table of contents\n*  [Introduction](#section1) \n*  [Read in the data](#section2)\n*  [Cleaning](#section3)\n    - [Numeric vs. non-numeric columns](#section4)\n    - [Missing values](#section5)\n    - [Normalization](#section6)\n*  [Univariate model](#section7)\n    - [Definition](#section8)\n    - [Testing](#section9)\n    - [Results](#section10)\n*  [Multivariate model](#section11)\n    - [Definition](#section12)\n    - [Top 5 features](#section13)\n    - [Testing](#section14)\n        - [Feature selection](#section15)\n        - [Hyperparameter tuning](#section16)\n    - [Results](#section17)\n*  [Cross validation](#section18) \n    - [Feature selection](#section19)\n    - [Hyperparameter tuning](#section20)\n    - [Results](#section21)\n    \n    by @antosnj"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "613f63cf5f49e01afcdef3792c294aa064b4cef8"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='section1'></a>\n# Introduction\nThis project aims to predict car prices using different K-Nearest Neighbors models. The data is sourced from https://archive.ics.uci.edu/ml/datasets/automobile."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb739094c4080e2ab0226d384c06836d788830e3"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\n\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "37872035a8a2f9ba7c421517c2c708299def86eb"
      },
      "cell_type": "markdown",
      "source": "<a id='section2'></a>\n# Read in the data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e82d587bae738b7e055ed06951e068f3cad74d87"
      },
      "cell_type": "code",
      "source": "cols = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ncars = pd.read_csv('../input/imports-85.data.txt', names=cols)\nprint(cars.shape)\ncars.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a72f17bd5ea8e76661403ee30b7149fa43e32ea"
      },
      "cell_type": "code",
      "source": "cars.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3b1b630d65d70f3475c0614505b0136fded7f73f"
      },
      "cell_type": "markdown",
      "source": "<a id='section3'></a>\n# Cleaning"
    },
    {
      "metadata": {
        "_uuid": "28a72fd586d20066088cf6303b06782529f761f3"
      },
      "cell_type": "markdown",
      "source": "----\n <a id='section4'></a>\n## Numeric vs. non-numeric columns\nBefore selecting the features to use for the model, let's see which ones are numeric. \n\nIn this case, referring to the Attribute Information of the dataset, found at https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names, and only selecting numeric columns with continuous values results in the most effective way to achieve this."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdb0f537b6917649f520c5947ead25c842eb5e22"
      },
      "cell_type": "code",
      "source": "continuous_numeric = ['normalized-losses', 'wheel-base', 'length', 'width', \n                      'height', 'curb-weight', 'bore', 'stroke', 'compression-ratio', \n                      'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n\nnumeric_cars = cars[continuous_numeric].copy()\nnumeric_cars.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c9ca53d2435d31f9256201b9a9ab84480006c50"
      },
      "cell_type": "markdown",
      "source": "<a id='section5'></a>\n## Missing values"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "3dfcd23c0cdcabf82dfa9605c6dfc967bb2204c1"
      },
      "cell_type": "code",
      "source": "numeric_cars.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ca75acdd3df4b4364b1f9678946dc4c1c614342"
      },
      "cell_type": "markdown",
      "source": "While there's not any NULL values in the cars dataframe, the 'normalized-losses' column contains 41 missing values, symbolized by a question mark '?', as seen below."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d77e6da79e906f6aecff9c2f4f037b3e334b0f54"
      },
      "cell_type": "code",
      "source": "numeric_cars['normalized-losses'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "127f91aa3efd34804b0abf6457d502f0c2832985"
      },
      "cell_type": "markdown",
      "source": "Let's replace any question mark in the data with the numpy.nan missing value."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d998d0930b4bfd5a1fbe2c9ed6f1674213de60b6"
      },
      "cell_type": "code",
      "source": "numeric_cars.replace('?', np.nan, inplace=True)\nprint(\"\\nMissing values before: \\n\\n\", numeric_cars.isnull().sum(), \"\\n\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf09bcf1d2968d7c1c63860a3020bdb3c595e5b1"
      },
      "cell_type": "markdown",
      "source": "Any column that now has NaN values on it, before containg question marks, which made pandas cast it to the object data types, as seen below."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b1300df06aafbfee086696bd4695e7658b7713a"
      },
      "cell_type": "code",
      "source": "numeric_cars.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88daebd1f3b985550b4f2991b7eee0f762d1f75c"
      },
      "cell_type": "markdown",
      "source": "Let's convert those to numeric types, since they all contain numeric data values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ddd5ac95975d13176721a65f56c293c4630510d"
      },
      "cell_type": "code",
      "source": "to_numeric_cols = ['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm', 'price']\nnumeric_cars[to_numeric_cols] = numeric_cars[to_numeric_cols].astype(float)\nnumeric_cars.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c151714e48e4c7ca237222688deca49646b296d4"
      },
      "cell_type": "markdown",
      "source": "The dataset has 205 rows, and we've seen how there're up to 41 NaN values. This means handling those by removing any row where there's a NaN value would result in losing close to 25% of the data, which is not a good solution.\n\nLet's only apply that to any row that has more than one missing value, and handle the rest by replacing any NaN value with the average of that column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47821328043b12f8c3260830890b9ae616a4ae94"
      },
      "cell_type": "code",
      "source": "numeric_cars.dropna(axis=0, thresh=2, inplace=True)\nnumeric_cars = numeric_cars.fillna(numeric_cars.mean())\nprint(\"\\nMissing values after: \\n\\n\", numeric_cars.isnull().sum(), \"\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e91bf1d73e63dee7c325b4f32d27348a856e1d5f"
      },
      "cell_type": "markdown",
      "source": "<a id='section6'></a>\n## Normalization\nNormalizing the numeric values using min-max normalization will make all values range from 0 to 1. This will prevent outliers when measuring squared erros.\n\nLet's apply that to all columns except for the target column, 'price'."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "023f1249d01b4ec2de983a019c3ff99a15bc3093"
      },
      "cell_type": "code",
      "source": "normalized_cars = (numeric_cars - numeric_cars.min())/(numeric_cars.max() - numeric_cars.min())\n#normalized_cars = np.abs((numeric_cars - numeric_cars.mean())/numeric_cars.std())\nnormalized_cars['price'] = numeric_cars['price']\nprint(normalized_cars.shape)\nnormalized_cars.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "03c8fe39c951133dfdc6f45d0f237ec1e9780528"
      },
      "cell_type": "markdown",
      "source": "<a id='section7'></a>\n# Univariate model"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0468884b21693dfcf78ad40a3ce81420aae1b970"
      },
      "cell_type": "markdown",
      "source": "The univariate model will use test/train validation, taking a single column as the selected feature, split the dataset into a training and test set, train and make predictions, returning the RMSE for the model.\n\nIt takes the training column name, target column name, the dataframe object, and a parameter for the _k_ value.\n\nIn this case, we'll consider splitting the dataset so that 50% of the rows represent the training set and the remaining 50% represent the test set.\n\nSince we want to predict car prices, we will use the 'price' column as the target column for the model."
    },
    {
      "metadata": {
        "_uuid": "3d7d55cdd4f6b310969497c090c61f11b98ea7df"
      },
      "cell_type": "markdown",
      "source": "<a id='section8'></a>\n## Definition"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fb79b0f87ce142d824c0e81907d56ac1a423da2"
      },
      "cell_type": "code",
      "source": "# Univariate model\ndef knn_train_test_uni(feature, target_column, df, k):\n    \n    # Randomize order of rows in data frame.\n    np.random.seed(1)\n    shuffled_index = np.random.permutation(df.index)\n    rand_df = df.reindex(shuffled_index)\n\n    # Split the dataset\n    train_set = rand_df.iloc[0:int(len(rand_df)/2)]\n    test_set = rand_df.iloc[int(len(rand_df)/2):]\n    \n    # Train\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(train_set[[feature]], train_set[target_column])\n    \n    # Predict\n    predictions = knn.predict(test_set[[feature]])\n    \n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(test_set[target_column], predictions))\n    \n    return rmse\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88f5caba4d9ebce1e7b62ac41ee7971d90ccf165"
      },
      "cell_type": "markdown",
      "source": "<a id='section9'></a>\n## Testing\nLet's test different models by changing:\n\n- The column used as feature: every numeric column (as previously stored in 'continuous_numeric').\n- The _k_ value: 1, 3, 5, 7 and 9.\n\nFor every numeric column, we'll try all k values, store and plot the RMSE results."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ca3af41a124668d481c4d5d62d727660014e727"
      },
      "cell_type": "code",
      "source": "k_values = [1, 3, 5, 7, 9]\nrmse_uni = {}\ncurrent_rmse = []\ntarget_column = 'price'\n\nfor feature in continuous_numeric[0:-1]:\n    for k in k_values:\n        current_rmse.append(knn_train_test_uni(feature, target_column, normalized_cars, k))\n        \n    rmse_uni[feature] = current_rmse\n    current_rmse = []\n\nrmse_uni",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7f1556e96575c0499ab9ff1797aeabc2bfa0450"
      },
      "cell_type": "markdown",
      "source": "<a id='section10'></a>\n## Results"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "8555c2894dd5f9e400e4ee8bd598464e503fa6a7"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1)\n\nfor key, values in rmse_uni.items():\n    ax.plot(k_values, values, label=key)\n    ax.set_xlabel('k value')\n    ax.set_ylabel('RMSE')\n    ax.set_title('RMSE for Each Training Column\\nvs. k value')\n    ax.tick_params(top=\"off\", left=\"off\", right=\"off\", bottom='off')\n    ax.legend(bbox_to_anchor=(1.5, 1), prop={'size': 11})\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "041b83e1eaf91d1cdd79b3e0a660c64e35509a39"
      },
      "cell_type": "markdown",
      "source": "We can see how the RMSE values range from 4,000 up to 11,000 dollars. \n\nLater on in this project, I will select the top 5 features based on the average of all RMSE values for each _k_ value, to be used to test different multivariate models, defined in the next section."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "58e40232704d8287cd74bca86ec66b0d88f68a8c"
      },
      "cell_type": "markdown",
      "source": "<a id='section11'></a>\n# Multivariate model\nThe multivariate model will perform the exact same way as the univariate, but will take a list of column names to be used as features."
    },
    {
      "metadata": {
        "_uuid": "25926722429afb9fa49d03cf588cf1050f72a3c5"
      },
      "cell_type": "markdown",
      "source": "<a id='section12'></a>\n## Definition"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da412b605779638e28c5876a71dea42399aad208"
      },
      "cell_type": "code",
      "source": "# Multivariate model\ndef knn_train_test(features, target_column, df, k):\n    \n    # Randomize order of rows in data frame.\n    np.random.seed(1)\n    shuffled_index = np.random.permutation(df.index)\n    rand_df = df.reindex(shuffled_index)\n\n    # Split the dataset\n    train_set = rand_df.iloc[0:int(len(rand_df)/2)]\n    test_set = rand_df.iloc[int(len(rand_df)/2):]\n    \n    # Train\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(train_set[features], train_set[target_column])\n    \n    # Predict\n    predictions = knn.predict(test_set[features])\n    \n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(test_set[target_column], predictions))\n    \n    return rmse\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d07e44b7f1789be55f08919408e0be9e3ce9084"
      },
      "cell_type": "markdown",
      "source": "<a id='section13'></a>\n## Top 5 features\nIn order to effectively test different set of features for the multivariate model, using sets of the top 5 from the univariate model could be a good approach.\n\nWe can achieve this by computing the average of all RMSE values got for each _k_ value, and assign that to every column that's been used."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c0bd1b3e01c605d28ebd33361f0f6ddfe7c1154"
      },
      "cell_type": "code",
      "source": "avg_rmse = {}\n\nfor key, values in rmse_uni.items():\n    avg_rmse[key] = np.mean(values)\n\navg_rmse = pd.Series(avg_rmse)\navg_rmse.sort_values()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90c2603d6c22d9c07da9be564d00f6613d447516"
      },
      "cell_type": "markdown",
      "source": "The top 5 features using this method are 'highway-mpg', 'curb-weight', 'horsepower', 'width' and 'city-mpg'."
    },
    {
      "metadata": {
        "_uuid": "225528d293f324c192f229dc58f4ca74da392b78"
      },
      "cell_type": "markdown",
      "source": "<a id='section14'></a>\n## Testing\n<a id='section15'></a>\n### Feature selection\nTo test different multivariate models, I will select the best 2, 3, 4, 5 and lastly 6 best features of the previous univariate 'RMSE ranking', and see which set of features performs best for the default _k_ value."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7862beb4a8b33ba953206f40d081d4cc5ecfcb71"
      },
      "cell_type": "code",
      "source": "features = {\n        'best_2': ['highway-mpg', 'curb-weight'],\n        'best_3': ['highway-mpg', 'curb-weight', 'horsepower'],\n        'best_4': ['highway-mpg', 'curb-weight', 'horsepower', 'width'],\n        'best_5': ['highway-mpg', 'curb-weight', 'horsepower', 'width', 'city-mpg'],\n        'best_6': ['highway-mpg', 'curb-weight', 'horsepower', 'width', 'city-mpg', 'length']\n    } \n\nrmse_multi = {}\ntarget_column = 'price'\nk = 5\n\nfor key, value in features.items():\n    rmse_multi[key] = knn_train_test(value, target_column, normalized_cars, k)\n    \npd.Series(rmse_multi).sort_values()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1fa6c0be88f47383e4e5080a07cbcf3bfb85a53b"
      },
      "cell_type": "markdown",
      "source": "<a id='section16'></a>\n### Hyperparameter tuning\nFrom the top 3 models in the last section (those using 'best_6', 'best_2', and 'best_3' as features), let's see how they perform when tuning the _k_ value from 1 to 25."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49a2be4af88464dd54d5154748776d7fb9506b4f"
      },
      "cell_type": "code",
      "source": "top_models = {\n        'best_2': ['highway-mpg', 'curb-weight'],\n        'best_3': ['highway-mpg', 'curb-weight', 'horsepower'],\n        'best_6': ['highway-mpg', 'curb-weight', 'horsepower', 'width', 'city-mpg', 'length']\n    } \n\nk_values = list(range(1, 26))\nrmse_multi_k = {}\nrmse_current = []\n\nfor key, value in top_models.items():\n    for k in k_values:\n        rmse_current.append(knn_train_test(value, target_column, normalized_cars, k))\n        \n    rmse_multi_k[key] = rmse_current\n    rmse_current = []\n    \nprint(rmse_multi_k)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "027312d153a3fe19128abcec7b224afabe005e61"
      },
      "cell_type": "markdown",
      "source": "<a id='section17'></a>\n## Results"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17f6212064575c530b03e04ed8e7a35dbbe3f86b"
      },
      "cell_type": "code",
      "source": "# Returns a dict with the min value of every key's list and its index the list\ndef min_key_value(dictionary):\n    min_values = {}\n    for k, v in dictionary.items():\n        min_values[k] = [min(v), v.index(min(v))]\n        \n    return min_values\n        \nbest_k = min_key_value(rmse_multi_k)\nprint(best_k)\n\n# Plot results\nfig, ax = plt.subplots(1)\n\nfor key, values in rmse_multi_k.items():\n    ax.plot(k_values, values, label=key)\n    ax.set_xlabel('k value')\n    ax.set_ylabel('RMSE')\n    ax.set_title('RMSE for Top 3 Models vs. k value\\n Test/Train Validation')\n    ax.tick_params(top=\"off\", left=\"off\", right=\"off\", bottom='off')\n    ax.legend(bbox_to_anchor=(1.5, 1), prop={'size': 11})\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5abb6b616a81b7755ec9810ad0cad734e82516d1"
      },
      "cell_type": "markdown",
      "source": "The best results for each model are:\n\n- 'best_6': RMSE of **3303.16** dollars, for **k=1**\n- 'best_3': RMSE of **3382.88** dollars, for **k=2**\n- 'best_2': RMSE of **3802.87** dollars, for **k=6**\n\nAll of them tend to perform worse as k increases from a certain point (between k=5 and k=10 approximately). \n\nIt also seems like the more features, the lower _k_ value that performs best. This may be because more features make entries (cars in this case) more unique, which means if a new car has a lot of attributes or features that make it distinct, it will be harder for it to be similar to a big number of cars in our training set (i.e. selecting a big _k_ when using many features may result in worse predictions).\n\nAs an example, that may be why the case of _k=1_ for the 'best_6' model (6 attributes/features) performs best, since even though every new car only chooses one similar/close in distance car (neighbor) from the training set, that one car is the most similar cosindering a big number of attributes, which results in a a good prediction."
    },
    {
      "metadata": {
        "_uuid": "5e0cccad6ebc685288e63160140f7cab15ed4e1f"
      },
      "cell_type": "markdown",
      "source": "<a id='section18'></a>\n# Cross validation\nLastly, for the multivariate model let's modify the knn_train_test() function to use k-fold cross validation instead of test/train validation and see how it performs for 10 folds."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c329a57f2ac683be1ec7a4999339f8e5e8bd6f7"
      },
      "cell_type": "code",
      "source": "def knn_cross_validation(features, target_column, df, k): \n    knn = KNeighborsRegressor(n_neighbors=k)\n    kf = KFold(n_splits=10, shuffle=True, random_state=1)\n    mses = cross_val_score(knn, df[features], df[target_column], scoring='neg_mean_squared_error', cv=kf)\n    avg_rmse = np.mean(np.sqrt(np.absolute(mses)))\n    \n    return avg_rmse\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0bc53ec15b680b8975f90a2942326735354530e"
      },
      "cell_type": "markdown",
      "source": "<a id='section19'></a>\n## Feature selection\nProceeding the exact same way as before, using the same selected features, the RMSE values returned are:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9343aa89c669b85b5e24cad6127736fc7d1b6d1"
      },
      "cell_type": "code",
      "source": "features = {\n        'best_2': ['highway-mpg', 'curb-weight'],\n        'best_3': ['highway-mpg', 'curb-weight', 'horsepower'],\n        'best_4': ['highway-mpg', 'curb-weight', 'horsepower', 'width'],\n        'best_5': ['highway-mpg', 'curb-weight', 'horsepower', 'width', 'city-mpg'],\n        'best_6': ['highway-mpg', 'curb-weight', 'horsepower', 'width', 'city-mpg', 'length']\n    } \n\nrmse_multi = {}\ntarget_column = 'price'\nk = 5\n\nfor key, value in features.items():\n    rmse_multi[key] = knn_cross_validation(value, target_column, normalized_cars, k)\n    \npd.Series(rmse_multi).sort_values()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0904d8ec0888f5fbe33665b40ab0fbead1a1962"
      },
      "cell_type": "markdown",
      "source": "This time, the 3 best models in terms of RMSE were 'best_4', 'best_3', and 'best_5'. \n\nPerforming hyperparameter tuning will tell us what the optimal _k_ value is for each of them, just as we did before."
    },
    {
      "metadata": {
        "_uuid": "b7c0a920ddb50cfb999a57fe97fbd8bd5cf7075f"
      },
      "cell_type": "markdown",
      "source": "<a id='section20'></a>\n## Hyperparameter tuning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f76fd3512f0972ad22956dc62cae82e8d4eed6d"
      },
      "cell_type": "code",
      "source": "top_models = {\n        'best_3': ['highway-mpg', 'curb-weight', 'horsepower'],\n        'best_4': ['highway-mpg', 'curb-weight', 'horsepower', 'width'],\n        'best_5': ['highway-mpg', 'curb-weight', 'horsepower', 'width', 'city-mpg']\n    } \n\nk_values = list(range(1, 26))\nrmse_multi_k = {}\nrmse_current = []\n\nfor key, value in top_models.items():\n    for k in k_values:\n        rmse_current.append(knn_cross_validation(value, target_column, normalized_cars, k))\n        \n    rmse_multi_k[key] = rmse_current\n    rmse_current = []\n    \nprint(rmse_multi_k)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "366142ddf81b5fc8ae406862b1ecb8f080126177"
      },
      "cell_type": "markdown",
      "source": "<a id='section21'></a>\n## Results"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e453ad11fc33342a9dc44e93211aff817891fc14"
      },
      "cell_type": "code",
      "source": "best_k = min_key_value(rmse_multi_k)\nprint(best_k)\n\n# Plot results\nfig, ax = plt.subplots(1)\n\nfor key, values in rmse_multi_k.items():\n    ax.plot(k_values, values, label=key)\n    ax.set_xlabel('k value')\n    ax.set_ylabel('RMSE')\n    ax.set_title('RMSE for Top 3 Models vs. k value\\n 10-Fold Cross Validation')\n    ax.tick_params(top=\"off\", left=\"off\", right=\"off\", bottom='off')\n    ax.legend(bbox_to_anchor=(1.5, 1), prop={'size': 11})\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d4546a4944803a388386eed117fa9719d09a6464"
      },
      "cell_type": "markdown",
      "source": "Using k-fold cross validation, the trend is similar to what we got before using test/train validation. However, this time we got better results:\n\n- 'best_3': RMSE of **2824.09** dollars, for **k=2**\n- 'best_4': RMSE of **3035.18** dollars, for **k=3**\n- 'best_5': RMSE of **3159.23** dollars, for **k=3**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff9e960fda7dab8444b8c6f719bf4fd11800f142"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}